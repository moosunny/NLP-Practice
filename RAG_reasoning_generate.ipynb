{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.0\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)  # PyTorch ë²„ì „ í™•ì¸\n",
    "print(torch.backends.mps.is_available())  # MPS ì§€ì› ì—¬ë¶€ í™•ì¸\n",
    "print(torch.backends.mps.is_built())  # MPSê°€ ë¹Œë“œë˜ì—ˆëŠ”ì§€ í™•ì¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "# reasoningì„ ìˆ˜í–‰í•˜ëŠ” llm\n",
    "reasoning_llm = ChatOllama(\n",
    "    model = 'deepseek-r1:7b',\n",
    "    stop = [\"</think>\"]\n",
    ")\n",
    "\n",
    "# ë‹µë³€ì„ ìƒì„±í•˜ëŠ” llm\n",
    "answer_llm = ChatOllama(\n",
    "    model = 'exaone3.5',\n",
    "    temperature = 0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated, List, TypedDict, Literal\n",
    "from langgraph.graph.message import add_messages\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "class RAGState(TypedDict):\n",
    "    query: str\n",
    "    thinking: str\n",
    "    Document: List[Document]\n",
    "    answer: str\n",
    "    messages: Annotated[List, add_messages]\n",
    "    mode: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_docling import DoclingLoader\n",
    "from langchain_docling.loader import ExportType\n",
    "\n",
    "FILE_PATH = \"https://arxiv.org/pdf/2106.09685\"\n",
    "\n",
    "loader = DoclingLoader(\n",
    "    file_path = FILE_PATH,\n",
    "    export_type = ExportType.MARKDOWN\n",
    ")\n",
    "\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-d.page_content='Edward Hu  \\nâˆ—  \\nYelong Shen  \\nâˆ—  \\nPhillip Wallis Zeyuan Allen-Zhu Lu Wang Weizhu Chen  \\nYuanzhi Li  \\nShean Wang  \\nMicrosoft Corporation  \\n{ edwardhu, yeshe, phwallis, zeyuana, yuanzhil, swang, luw, wzchen @microsoft.com } yuanzhil@andrew.cmu.edu  \\n(Version 2)'\n",
      "-d.page_content='An important paradigm of natural language processing consists of large-scale pretraining on general domain data and adaptation to particular tasks or domains. As we pre-train larger models, full fine-tuning, which retrains all model parameters, becomes less feasible. Using GPT-3 175B as an example - deploying independent instances of fine-tuned models, each with 175B parameters, is prohibitively expensive. We propose Lo wR ank A daptation, or LoRA, which freezes the pretrained model weights and injects trainable rank decomposition matrices into each layer of the Transformer architecture, greatly reducing the number of trainable parameters for downstream tasks. Compared to GPT-3 175B fine-tuned with Adam, LoRA can reduce the number of trainable parameters by 10,000 times and the GPU memory requirement by 3 times. LoRA performs on-par or better than finetuning in model quality on RoBERTa, DeBERTa, GPT-2, and GPT-3, despite having fewer trainable parameters, a higher training throughput, and, unlike adapters, no additional inference latency . We also provide an empirical investigation into rank-deficiency in language model adaptation, which sheds light on the efficacy of LoRA. We release a package that facilitates the integration of LoRA with PyTorch models and provide our implementations and model checkpoints for RoBERTa, DeBERTa, and GPT-2 at https://github.com/microsoft/LoRA .'\n",
      "-d.page_content=\"Many applications in natural language processing rely on adapting one large-scale, pre-trained language model to multiple downstream applications. Such adaptation is usually done via fine-tuning , which updates all the parameters of the pre-trained model. The major downside of fine-tuning is that the new model contains as many parameters as in the original model. As larger models are trained every few months, this changes from a mere 'inconvenience' for GPT-2 (Radford et al., b) or RoBERTa large (Liu et al., 2019) to a critical deployment challenge for GPT-3 (Brown et al., 2020) with 175 billion trainable parameters. 1  \\nMany sought to mitigate this by adapting only some parameters or learning external modules for new tasks. This way, we only need to store and load a small number of task-specific parameters in addition to the pre-trained model for each task, greatly boosting the operational efficiency when deployed. However, existing techniques  \\nFigure 1: Our reparametrization. We only train A and B .  \\nf(x)  \\nPretrained  \\nWeights  \\nğ‘Š âˆˆ â„ ğ‘‘Ã—ğ‘‘  \\nx  \\nğ‘‘  \\noften introduce inference latency (Houlsby et al., 2019; Rebuffi et al., 2017) by extending model depth or reduce the model's usable sequence length (Li &amp; Liang, 2021; Lester et al., 2021; Hambardzumyan et al., 2020; Liu et al., 2021) (Section 3). More importantly, these method often fail to match the fine-tuning baselines, posing a trade-off between efficiency and model quality.  \\nWe take inspiration from Li et al. (2018a); Aghajanyan et al. (2020) which show that the learned over-parametrized models in fact reside on a low intrinsic dimension. We hypothesize that the change in weights during model adaptation also has a low 'intrinsic rank', leading to our proposed Lo wR ank A daptation (LoRA) approach. LoRA allows us to train some dense layers in a neural network indirectly by optimizing rank decomposition matrices of the dense layers' change during adaptation instead, while keeping the pre-trained weights frozen, as shown in Figure 1. Using GPT-3 175B as an example, we show that a very low rank (i.e., r in Figure 1 can be one or two) suffices even when the full rank (i.e., d ) is as high as 12,288, making LoRA both storage- and compute-efficient.  \\nLoRA possesses several key advantages.  \\n- Â· A pre-trained model can be shared and used to build many small LoRA modules for different tasks. We can freeze the shared model and efficiently switch tasks by replacing the matrices A and B in Figure 1, reducing the storage requirement and task-switching overhead significantly.\\n- Â· LoRA makes training more efficient and lowers the hardware barrier to entry by up to 3 times when using adaptive optimizers since we do not need to calculate the gradients or maintain the optimizer states for most parameters. Instead, we only optimize the injected, much smaller low-rank matrices.\\n- Â· Our simple linear design allows us to merge the trainable matrices with the frozen weights when deployed, introducing no inference latency compared to a fully fine-tuned model, by construction.\\n- Â· LoRA is orthogonal to many prior methods and can be combined with many of them, such as prefix-tuning. We provide an example in Appendix E.  \\nTerminologies and Conventions We make frequent references to the Transformer architecture and use the conventional terminologies for its dimensions. We call the input and output dimension size of a Transformer layer d model . We use W q , W k , W v , and W o to refer to the query/key/value/output projection matrices in the self-attention module. W or W 0 refers to a pretrained weight matrix and âˆ† W its accumulated gradient update during adaptation. We use r to denote the rank of a LoRA module. We follow the conventions set out by (Vaswani et al., 2017; Brown et al., 2020) and use Adam (Loshchilov &amp; Hutter, 2019; Kingma &amp; Ba, 2017) for model optimization and use a Transformer MLP feedforward dimension d ffn = 4 Ã— d model .\"\n",
      "...\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import MarkdownHeaderTextSplitter\n",
    "\n",
    "splitter = MarkdownHeaderTextSplitter(\n",
    "    headers_to_split_on = [\n",
    "        (\"#\", \"Header_1\"),\n",
    "        (\"##\", \"Header_2\"),\n",
    "        (\"###\", \"Header_3\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "splits = [split for doc in docs for split in splitter.split_text(doc.page_content)]\n",
    "\n",
    "for d in splits[:3]:\n",
    "    print(f\"-{d.page_content=}\")\n",
    "print(\"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Having shown that LoRA can be a competitive alternative to full fine-tuning on NLU, we hope to answer if LoRA still prevails on NLG models, such as GPT-2 medium and large (Radford et al., b). We keep our setup as close as possible to Li &amp; Liang (2021) for a direct comparison. Due to space constraint, we only present our result on E2E NLG Challenge (Table 3) in this section. See Section F.1 for results on WebNLG (Gardent et al., 2017) and DART (Nan et al., 2020). We include a list of the hyperparameters used in Section D.3.  \n",
       "Table 4: Performance of different adaptation methods on GPT-3 175B. We report the logical form validation accuracy on WikiSQL, validation accuracy on MultiNLI-matched, and Rouge-1/2/L on SAMSum. LoRA performs better than prior approaches, including full fine-tuning. The results on WikiSQL have a fluctuation around Â± 0 5% . , MNLI-m around Â± 0 1% . , and SAMSum around Â± 0 2 . / Â± 0 2 . / Â± 0 1 . for the three metrics.  \n",
       "| Model&Method       | # Trainable Parameters   |   WikiSQL Acc. (%) |   MNLI-m Acc. (%) | SAMSum R1/R2/RL   |\n",
       "|--------------------|--------------------------|--------------------|-------------------|-------------------|\n",
       "| GPT-3 (FT)         | 175,255.8M               |               73.8 |              89.5 | 52.0/28.0/44.5    |\n",
       "| GPT-3 (BitFit)     | 14.2M                    |               71.3 |              91   | 51.3/27.4/43.5    |\n",
       "| GPT-3 (PreEmbed)   | 3.2M                     |               63.1 |              88.6 | 48.3/24.2/40.5    |\n",
       "| GPT-3 (PreLayer)   | 20.2M                    |               70.1 |              89.5 | 50.8/27.3/43.5    |\n",
       "| GPT-3 (Adapter H ) | 7.1M                     |               71.9 |              89.8 | 53.0/28.9/44.8    |\n",
       "| GPT-3 (Adapter H ) | 40.1M                    |               73.2 |              91.5 | 53.2/29.0/45.1    |\n",
       "| GPT-3 (LoRA)       | 4.7M                     |               73.4 |              91.7 | 53.8/29.8/45.9    |\n",
       "| GPT-3 (LoRA)       | 37.7M                    |               74   |              91.6 | 53.4/29.2/45.1    |"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Markdown\n",
    "\n",
    "display(Markdown(splits[12].page_content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import OllamaEmbeddings\n",
    "\n",
    "embeddings = OllamaEmbeddings(\n",
    "    model = \"bge-m3:latest\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_qdrant import QdrantVectorStore\n",
    "from langchain_qdrant import RetrievalMode\n",
    "\n",
    "vector_store = QdrantVectorStore.from_documents(\n",
    "    documents=splits,\n",
    "    embedding=embeddings,\n",
    "    location = \":memory:\",\n",
    "    collectio_name = \"rag_collection_0228\",\n",
    "    retrieval_mode = RetrievalMode.DENSE\n",
    ")\n",
    "\n",
    "retriver = vector_store.as_retriever(search_kwargs = {'k':10})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import CrossEncoderReranker\n",
    "from langchain_community.cross_encoders import HuggingFaceCrossEncoder\n",
    "\n",
    "model = HuggingFaceCrossEncoder(model_name = \"BAAI/bge-reranker-base\")\n",
    "compressor = CrossEncoderReranker(model = model, top_n= 5)\n",
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=compressor, base_retriever=retriver\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "from langgraph.graph import START, StateGraph, END\n",
    "\n",
    "def classify_node(state: RAGState):\n",
    "    \"\"\"ì§ˆë¬¸ì„ ë¶„ë¥˜í•˜ì—¬ ì²˜ë¦¬ ëª¨ë“œë¥¼ ê²°ì •\"\"\"\n",
    "    query = state[\"query\"]\n",
    "    if \"Docling\" in query:\n",
    "        print(\"====ê²€ìƒ‰ ì‹œì‘=====\")\n",
    "        return {\"mode\": \"retrieve\"}\n",
    "    else:\n",
    "        print(\"=====ìƒì„± ì‹œì‘=====\")\n",
    "        return {\"mode\": \"generate\"}\n",
    "\n",
    "def route_by_mode(state:RAGState) -> Literal[\"retrieve\", \"generate\"]:\n",
    "    \"\"\"ëª¨ë“œì— ë”°ë¼ ë‹¤ìŒ ë‹¨ê³„ë¥¼ ê²°ì •í•©ë‹ˆë‹¤.\"\"\"\n",
    "    return state[\"mode\"]\n",
    "\n",
    "def retrieve(state:RAGState):\n",
    "    \"\"\"ì§ˆì˜ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ê´€ë ¨ ë¬¸ì„œë¥¼ ê²€ìƒ‰\"\"\"\n",
    "    query = state[\"query\"]\n",
    "    print(\"===ê²€ìƒ‰ ì‹œì‘===\")\n",
    "    documents = compression_retriever.invoke(query)\n",
    "    for doc in documents:\n",
    "        print(doc.page_content)\n",
    "        print(\"-\"*100)\n",
    "    print(\"====ê²€ìƒ‰ ì™„ë£Œ====\")\n",
    "    return {\"documents\": documents}\n",
    "\n",
    "def reasoning(state: RAGState):\n",
    "    \"\"\"\"ì¿¼ë¦¬ë¥¼ ë¶„ì„í•˜ê³  ì‚¬ê³  ê³¼ì •ì„ ìƒì„±í•©ë‹ˆë‹¤.\"\"\"\n",
    "    query = state[\"query\"]\n",
    "    documents = state[\"documents\"]\n",
    "    context = \"\\n\\n\".join([doc.page_content for doc in documents])\n",
    "    reasoning_prompt = ChatPromptTemplate.from_template(\n",
    "        \"\"\"ì£¼ì–´ì§„ ë¬¸ì„œë¥¼ í™œìš©í•˜ì—¬ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ê°€ì¥ ì ì ˆí•œ ë‹µë³€ì„ ì‘ì„±í•´ì£¼ì„¸ìš”.\n",
    "        \n",
    "        ì§ˆë¬¸: {query}\n",
    "        ë¬¸ì„œ ë‚´ìš©:\n",
    "        {context}\n",
    "\n",
    "        ìƒì„¸ ì¶”ë¡ :\"\"\"\n",
    "    )\n",
    "\n",
    "    reasoning_chain = reasoning_prompt | reasoning_llm | StrOutputParser()\n",
    "    print(\"====ì¶”ë¡  ì‹œì‘====\")\n",
    "    thinking = reasoning_chain.invoke({\"query\": query, \"context\": context})\n",
    "\n",
    "    return {\"thinkinh\": thinking}\n",
    "\n",
    "def generate(state:RAGState):\n",
    "    \"\"\"ë¬¸ì„œì™€ ì¶”ë¡  ê³¼ì •ì„ ê¸°ë°˜ìœ¼ë¡œ ìµœì¢… ë‹µë³€ì„ ìƒì„±\"\"\"\n",
    "    query = state[\"query\"]\n",
    "    thinking = state[\"thinking\"]\n",
    "    documents = state[\"documents\"]\n",
    "\n",
    "    context = \"\\n\\n\".join([doc.page_content for doc in documents])\n",
    "\n",
    "    answer_prompt = ChatPromptTemplate.from_template(\n",
    "        \"\"\"ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— í•œê¸€ë¡œ ë‹µë³€í•˜ì„¸ìš”. ì œê³µëœ ë¬¸ì„œì™€ ì¶”ë¡  ê³¼ì •ì´ ìˆë‹¤ë©´, ìµœëŒ€í•œ í™œìš©í•˜ì„¸ìš”.\n",
    "        \n",
    "        ì§ˆë¬¸: \n",
    "        {query}\n",
    "\n",
    "        ì¶”ë¡  ê³¼ì •:\n",
    "        {thinking}\n",
    "\n",
    "        ë¬¸ì„œ ë‚´ìš©:\n",
    "        {context}\n",
    "\n",
    "        ë‹µë³€:\"\"\"\n",
    "    )\n",
    "    print(\"=====ë‹µë³€ ìƒì„± ì™„ë£Œ====\")\n",
    "    answer_chain = answer_prompt | answer_llm | StrOutputParser\n",
    "    answer = answer_chain.invoke({\n",
    "        \"query\": query,\n",
    "        \"thingking\": thinking,\n",
    "        \"context\": context\n",
    "    })\n",
    "    return {\n",
    "        \"answer\": answer,\n",
    "        \"messages\": [HumanMessage(content = answer)]\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x31a3f0e90>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "workflow = StateGraph(RAGState)\n",
    "\n",
    "workflow.add_node(\"classify\", classify_node)\n",
    "workflow.add_node(\"reasoning\", reasoning)\n",
    "workflow.add_node(\"retrieve\", retrieve)\n",
    "workflow.add_node(\"generate\", generate)\n",
    "\n",
    "workflow.add_edge(START, \"classify\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"classify\",\n",
    "    route_by_mode,\n",
    "    {\"retrieve\": \"retrieve\",\n",
    "     \"generate\": \"generate\"}\n",
    ") \n",
    "\n",
    "workflow.add_edge(\"retrieve\", \"reasoning\")\n",
    "workflow.add_edge(\"reasoning\", \"generate\")\n",
    "workflow.add_edge(\"generate\", END)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Deeplearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
